%!TEX root = BiSig.tex

\section{Mode Preprocessing and Related Properties}\label{sec:pre-synthesis}

Our first important construction is mode preprocessing in \cref{sec:mode-preprocessing}, which is in fact generalised to pinpoint any missing type annotations in a given raw term.
We also discuss some related properties:
By bringing mode derivations into the picture, we are able to give a natural formulation of soundness and completeness of a bidirectional type system with respect to its erasure to an ordinary type system in \cref{sec:soundness-and-completeness}.
We also formulate \varcitet{Dunfield2021}{'s} annotatability and discuss its relationship with our completeness and generalised mode preprocessing in \cref{sec:annotatability}.

\subsection{Soundness and Completeness}
\label{sec:soundness-and-completeness}

%\Josh{Throughout this sub-section, we quantify over any bidirectional type system~$(\Sigma, \Omega)$, context $\Gamma : \Cxt_\Sigma(\emptyset)$, raw term $\erase\Gamma |-_{\Sigma, \erase\Omega} t$, mode~$d$, and type $A : \Type_\Sigma(\emptyset)$.}

Erasure of a bidirectional binding signature removes mode information and keeps everything else intact; this can be straightforwardly extended by induction to remove mode information from a bidirectional typing derivation and arrive at an ordinary typing derivation, which is soundness.
We can also remove typing and retain mode information, arriving at a mode derivation instead.
Conversely, if we have both mode and typing derivations for the same term, we can combine them and obtain a bidirectional typing derivation, which is completeness.
In short, soundness and completeness are no more than the separation and combination of mode and typing information carried by the three kinds of derivations while keeping their basic structure, which is directed by the same raw term.
%Formulating the notion of mode derivation helps to complete this clean picture.
Technically, all these can be summarised in one theorem.

\begin{theorem}\label{lem:soundness-completeness}
$\Gamma |-_{\Sigma, \Omega} t :^\dir{d} A$ if and only if both\/ $\erase\Gamma |-_{\Sigma, \Omega} t^\dir{d}$ and\/ $\Gamma |-_{\Sigma, \erase\Omega} t : A$.
\end{theorem}

\begin{proof}
From left to right:
Induction on the bidirectional typing derivation, mapping every rule to its counterpart except when constructing a typing derivation from the $\ChkRule{Sub}$ rule, in which case the induction hypothesis $\Gamma |-_{\Sigma,\erase\Omega} t : B$ suffices due to the premise $B = A$.

From right to left:
Induction on the mode derivation.
For \SynRule{Var}, \SynRule{Anno}, and \Rule{Op}, the outermost rule used in the typing derivation must be the corresponding typing rule, so by the induction hypotheses we have bidirectional typing derivations for all the sub-terms, to which we can then apply the corresponding bidirectional typing rule.
The \ChkRule{Sub} case is similar but slightly simpler: the induction hypothesis directly gives us a derivation of $\Gamma |-_{\Sigma,\Omega} t \syn A$, to which we apply $\ChkRule{Sub}$.
\end{proof}


\subsection{Generalised Mode Preprocessing}
\label{sec:mode-preprocessing}

The goal of this sub-section is to construct a mode preprocessor, which decides for any raw term $V |-_{\Sigma,\erase\Omega} t$ and mode~$\dir d$ whether $V |-_{\Sigma,\Omega} t^\dir{d}$ or not.
In fact we will do better:
If a mode preprocessor returns a proof that no mode derivation exists, that proof (of a negation) does not provide useful information for the user.
It will be more helpful if a preprocessor can produce an explanation of why no mode derivation exists, and even how to fix the input term to have a mode derivation.
We will construct such a \emph{generalised mode preprocessor}~(\cref{thm:generalised-mode-preprocessing}), which can be weakened to an ordinary mode preprocessor~(\cref{thm:mode-preprocessing}) if the additional explanation is not needed.%
\footnote{For clarity, we use ordinary mode preprocessing elsewhere in this paper.}

\begin{figure}
  \centering
  \small
  \begin{tabular}{ r r l }
    & & is in mode~$d$, \\
    $\smash{\boxed{V |-_{\Sigma, \Omega} \isTerm{t}^{\dir{d}\,g\,s}}}$
    & The raw term~$\isTerm{t}$\hspace{-.6em}
    & misses some type annotation iff $g = \BFalse$, and \\
    & & is in mode~$d$ due to an outermost mode cast iff $s = \BFalse$
  \end{tabular}
  \begin{mathpar}
    \inferrule{x \in V}{V |-_{\Sigma, \Omega} \isTerm{x}^{\syn\,\BTrue\,\BTrue}}\,\SynRule{Var}
    \and
    \inferrule{\cdot |-_{\Sigma} A \\ V |-_{\Sigma, \Omega}\isTerm{t}^{\chk\,g\,s}}{V |-_{\Sigma, \Omega} (\isTerm{t \annotate A})^{\syn\,g\,\BTrue}}\,\SynRule{Anno}
    \and
    \inferrule{V |-_{\Sigma, \Omega}\isTerm{t}^{\chk\,g\,\BTrue}}{V |-_{\Sigma, \Omega} \isTerm{t}^{\syn\,\BFalse\,\BFalse}}\,\SynRule{Missing}
    \and
    \inferrule{V |-_{\Sigma, \Omega} \isTerm{t}^{\syn\,g\,\BTrue}}{V |-_{\Sigma, \Omega} \isTerm{t}^{\chk\,g\,\BFalse}}\,\ChkRule{Sub}
    \and
    \inferrule{V, \vec x_1 |-_{\Sigma, \Omega} \isTerm{t_1}^{\dir{d_1}\,g_1\,s_1} \\ \cdots \\ V, \vec x_n |-_{\Sigma, \Omega} \isTerm{t_n}^{\dir{d_n}\,g_n\,s_n}}
    {V |-_{\Sigma, \Omega} \tmOpts^{\dir{d}\,(\bigwedge_i g_i)\,\BTrue}}\,\Rule{Op}
  \end{mathpar}
  \caption{Generalised mode derivations}
  \label{fig:generalised-mode-derivations}
\end{figure}

Intuitively, a term does not have a mode derivation exactly when there are not enough type annotations, but such negative formulations convey little information.
Instead, we can provide more information by pointing out the places in the term that require annotations.
For a bidirectional type system, an annotation is required wherever a term is `strictly' (which we will explain shortly) in checking mode but required to be in synthesising mode, in which case there is no rule for switching from checking to synthesising, and thus there is no way to construct a mode derivation.
We can, however, consider \emph{generalised mode derivations} defined in \cref{fig:generalised-mode-derivations} that allow the use of an additional $\SynRule{Missing}$ rule for such switching, so that a derivation can always be constructed.
Given a generalised mode derivation, if it uses $\SynRule{Missing}$ in some places, then those places are exactly where annotations should be supplied, which is helpful information to the user; if it does not use $\SynRule{Missing}$, then the derivation is \emph{genuine} in the sense that it corresponds directly to an original mode derivation.
This can be succinctly formulated as \cref{thm:Pre?-true} below by encoding genuineness as a boolean~$g$ in the generalised mode judgement, which is set to~$\BFalse$ only by the $\SynRule{Missing}$ rule.
(Ignore the boolean~$s$ for now.)

\begin{lemma}\label{thm:Pre?-true}
If\/ $V |-_{\Sigma,\Omega} t^{\dir{d}\,\BTrue\,s}$, then $V |-_{\Sigma,\Omega} t^\dir{d}$.
\end{lemma}

\begin{proof}
Induction on the given derivation.
The $\SynRule{Missing}$ rule cannot appear because $g = \BTrue$, and the other rules are mapped to their counterparts.
\end{proof}

We also want a lemma that covers the case where $g = \BFalse$.

\begin{lemma}\label{thm:Pre?-false}
If\/ $V |-_{\Sigma,\Omega} t^{\dir{d}\,\BFalse\,s}$, then $V |/-_{\Sigma,\Omega} t^\dir{d}$.
\end{lemma}

This lemma would be wrong if the `strictness' boolean~$s$ was left out of the rules:
Having both $\ChkRule{Sub}$ and $\SynRule{Missing}$, which we call \emph{mode casts}, it would be possible to switch between the two modes freely, which unfortunately means that we could insert a pair of $\ChkRule{Sub}$ and $\SynRule{Missing}$ anywhere, constructing a non-genuine derivation even when there is in fact a genuine one.
The `strictness' boolean~$s$ can be thought of as disrupting the formation of such pairs of mode casts:
Every rule other than the mode casts sets~$s$ to~$\BTrue$, meaning that a term is \emph{strictly} in the mode assigned by the rule (i.e.~not altered by a mode cast), whereas the mode casts set~$s$ to~$\BFalse$.
Furthermore, the sub-derivation of a mode cast has to be strict, so it is impossible to have consecutive mode casts.
Another way to understand the role of~$s$ is that it makes the $\SynRule{Missing}$ rule precise: an annotation is truly missing only when a term is \emph{strictly} in checking mode but is required to be in synthesising mode.
With strictness coming into play, non-genuine derivations are now `truly non-genuine'.

\begin{proof}[Proof of \cref{thm:Pre?-false}]
Induction on the generalised mode derivation, in each case analysing an arbitrary mode derivation and showing that it cannot exist.
The key case is $\SynRule{Missing}$, where we have a sub-derivation of $V |-_{\Sigma,\Omega} t^{\chk\,g\,\BTrue}$ for some boolean~$g$.
We do not have an induction hypothesis and seem to get stuck, because $g$~is not necessarily~$\BFalse$.
But what matters here is that $t$~is \emph{strictly} in checking mode: if we continue to analyse the sub-derivation, the outermost rule must be $\Rule{Op}$ with $\dir d = {\chk}$, implying that $t$~has to be an operation in checking mode.
Then a case analysis shows that it is impossible to have a (synthesising) mode derivation of $V |-_{\Sigma,\Omega} t^{\syn}$.
%\begin{itemize}
%\item $\SynRule{Anno}$:
%Any mode derivation must also use $\SynRule{Anno}$ as the outermost rule, and then the induction hypothesis suffices.
%\item $\ChkRule{Sub}$:
%We have a sub-derivation of $V |-_{\Sigma,\Omega} t^{\syn\,\BFalse\,\BTrue}$.
%Any mode derivation of $V |-_{\Sigma,\Omega} t^{\chk}$ must use $\ChkRule{Chk}$ as the outermost rule.
%(The $\Rule{Op}$ rule with $\dir d = {\chk}$ is impossible because the sub-derivation shows that $t$~is exactly a synthesising term.)
%We then have a sub-derivation of $V |-_{\Sigma,\Omega} t^{\syn}$, and can use the induction hypothesis.
%\end{itemize}
\end{proof}

Now we are ready to construct a generalised mode preprocessor.

\begin{theorem}[Generalised Mode Preprocessing]\label{thm:generalised-mode-preprocessing}
For any raw term $V |-_{\Sigma,\erase\Omega} t$ and mode~$\dir{d}$, there is a derivation of\/ $V |-_{\Sigma,\Omega} t^{\dir{d}\,g\,s}$ for some booleans $g$~and~$s$.
\end{theorem}

The theorem could be proved directly, but that would mix up two case analyses which respectively inspect the input term and apply mode casts depending on what modes are required.
Instead, we distill the case analysis that deals with mode casts into the following \cref{thm:adjustment}, whose antecedent~(\ref{eq:strict-derivation}) is then established by induction on the input term in the proof of \cref{thm:generalised-mode-preprocessing}.

\begin{lemma}\label{thm:adjustment}
For any raw term $V |-_{\Sigma,\erase\Omega} t$, if
\begin{equation}\label{eq:strict-derivation}
V |-_{\Sigma,\Omega} t^{\dir{d'}\,g'\,\BTrue} \quad\text{for some mode~$\dir{d'}$ and boolean~$g'$}
\end{equation}
then for any mode~$\dir{d}$, there is a derivation of\/ $V |-_{\Sigma,\Omega} t^{\dir{d}\,g\,s}$ for some booleans $g$~and~$s$.
\end{lemma}

\begin{proof}
Case analysis on $\dir{d}$ and $\dir{d'}$, adding an outermost mode cast to change the given derivation to a different mode if $d \neq d'$.
Note that it is permissible to add an outermost mode cast because the antecedent~(\ref{eq:strict-derivation}) requires the given derivation to be strict.
\end{proof}



\begin{proof}[Proof of \cref{thm:generalised-mode-preprocessing}]
By \cref{thm:adjustment}, it suffices to prove statement~(\ref{eq:strict-derivation}) by induction on~$t$.

\begin{itemize}
\item $\Rule{Var}$ is mapped to $\SynRule{Var}$.
\item $\Rule{Anno}$:
Let $t = (t' \bbcolon A)$.
By the induction hypothesis and \cref{thm:adjustment}, there is a derivation of $V |-_{\Sigma,\Omega} {t'}^{\chk\,g\,s}$ for some booleans $g$~and~$s$, to which we apply $\SynRule{Anno}$.
\item $\Rule{Op}$:
Let $t = \tmOpts$.
By the induction hypotheses and \cref{thm:adjustment}, there is a derivation of $V, \vec x_i |-_{\Sigma,\Omega} {t_i}^{\dir{d_i}\,g_i\,s_i}$ for each sub-term~$t_i$; apply $\Rule{Op}$ to all the derivations.
\vspace{-\topsep-\baselineskip}
\end{itemize}
\end{proof}

Finally, having constructed a generalised mode preprocessor, it is easy to derive an ordinary one.

\begin{corollary}[Mode Preprocessing]\label{thm:mode-preprocessing}
  It is decidable whether\/ $V |-_{\Sigma,\Omega} \isTerm{t}^\dir{d}$.%for any raw term $V |-_{\Sigma,\erase\Omega} t$ and mode~$\dir{d}$
\end{corollary}

\begin{proof}
By \cref{thm:generalised-mode-preprocessing}, there is a derivation of $V |-_{\Sigma,\erase\Omega} t^{\dir{d}\,g\,s}$ for some booleans $g$~and~$s$, and then simply check~$g$ and apply either \cref{thm:Pre?-true} or \cref{thm:Pre?-false} to obtain $V |-_{\Sigma,\Omega} t^\dir{d}$ or $V |/-_{\Sigma,\Omega} t^\dir{d}$.
\end{proof}

%We can also establish the converse of \cref{thm:Pre?-true,thm:Pre?-false}.
%
%\begin{corollary}\label{thm:toPre?-true}
%If\/ $V |-_{\Sigma,\Omega} t^\dir{d}$, then $V |-_{\Sigma,\Omega} t^{\dir{d}\,\BTrue\,s}$ for some boolean~$s$.
%\end{corollary}
%
%\begin{corollary}\label{thm:toPre?-false}
%If\/ $V |/-_{\Sigma,\Omega} t^\dir{d}$, then $V |-_{\Sigma,\Omega} t^{\dir{d}\,\BFalse\,s}$ for some boolean~$s$.
%\end{corollary}
%
%Whereas it is possible to prove \cref{thm:toPre?-true} directly by induction, converting a mode derivation to a generalised one, the negated antecedent of \cref{thm:toPre?-false} provides little information for constructing a required derivation, and basically we have to construct such a derivation from scratch for a raw term.
%But this is exactly what generalised mode preprocessing does, so we can simply reuse \cref{thm:generalised-mode-preprocessing}.
%And \cref{thm:toPre?-true} can also be proved easily in the same way.
%
%\begin{proof}[Proofs of \cref{thm:toPre?-true,thm:toPre?-false}]
%
%By \cref{thm:generalised-mode-preprocessing}, there is a derivation of $V |-_{\Sigma,\Omega} t^{\dir{d}\,g\,s}$ for some booleans $g$~and~$s$.
%If $V |-_{\Sigma,\Omega} t^\dir{d}$, then $g$~has to be $\BTrue$ since $g = \BFalse$ leads to a contradiction with \cref{thm:Pre?-false}.
%Symmetrically, if $V |/-_{\Sigma,\Omega} t^\dir{d}$, then $g$~has to be~$\BFalse$ so as not to contradict \cref{thm:Pre?-true}.
%\end{proof}

\subsection{Annotatability}
\label{sec:annotatability}

\begin{figure}
  \centering\small
  \judgbox{\isTerm{t} \sqsupseteq \isTerm{u}}{A raw term $t$ is more annotated than $u$ (for some bidirectional type system $(\Sigma, \Omega)$)}
  \begin{mathpar}
    \inferrule{\isTerm{t} \sqsupseteq \isTerm{u}}
              {(\isTerm{t} \annotate A) \sqsupseteq \isTerm{u}}\;\Rule{More}
    \and
    \inferrule{\vphantom{x : \Identifier}}
              {\isTerm{x} \sqsupseteq \isTerm{x}}
    \and
    \inferrule{\isTerm{t} \sqsupseteq \isTerm{u}}
              {(\isTerm{t} \annotate A) \sqsupseteq (\isTerm{u} \annotate A)}
    \and
    \inferrule{\isTerm{t_1} \sqsupseteq \isTerm{u_1} \quad \cdots \quad \isTerm{t_n} \sqsupseteq{u_n}}
              {\tmOpts \sqsupseteq \tmOpus}
  \end{mathpar}
  
  \caption{Annotation ordering between raw terms}
  \label{fig:annotation-ordering}
\end{figure}

\citet{Dunfield2021} formulated completeness differently from ours (\cref{thm:STLC-completeness,sec:soundness-and-completeness}) and proposed \emph{annotatability} as a more suitable name.
In our theory, we may formulate annotatability as follows.

\begin{proposition}[Annotatability]\label{thm:annotatability}
If\/ $\Gamma |-_{\Sigma,\erase\Omega} t : A$, then there exists~$t'$ such that\/ $t' \sqsupseteq t$ and $\Gamma |-_{\Sigma,\Omega} t' :^\dir{d} A$ for some~$\dir{d}$.
\end{proposition}

Defined in \cref{fig:annotation-ordering}, the `annotation ordering' $t' \sqsupseteq t$ means that $t'$~has the same or more annotations than~$t$.
In a sense, annotatability is a reasonable form of completeness: if a term is typable in $(\Sigma, \erase\Omega)$, it may not be typable in $(\Sigma, \Omega)$ directly due to some missing annotations, but will be if those annotations are added correctly.
In our theory, \cref{thm:annotatability} can be easily proved from generalised mode preprocessing.

\begin{proof}[Proof of \cref{thm:annotatability}]
By \cref{thm:generalised-mode-preprocessing}, there is a generalised mode derivation of $V |-_{\Sigma,\erase\Omega} t^{\dir{d}\,g\,s}$ for some mode~$\dir{d}$ and booleans $g$~and~$s$.
Perform induction on this generalised mode derivation to construct a bidirectional typing derivation in the same mode (and also the term~$t'$ and a proof of $t' \sqsupseteq t$, which are determined by the derivations and omitted here).
\begin{itemize}
\item $\SynRule{Var}$, $\SynRule{Anno}$, and $\Rule{Op}$:
The outermost rule of the given typing derivation must be the corresponding one; apply the corresponding bidirectional typing rule to the induction hypotheses.
\item $\ChkRule{Sub}$:
Apply $\ChkRule{Sub}$ to the induction hypothesis.
\item $\SynRule{Missing}$:
This is the interesting case where we map a $\SynRule{Missing}$ rule to an $\SynRule{Anno}$ rule and add to the term a type annotation, which comes from the given typing derivation.
\vspace{-\topsep-\baselineskip}
\end{itemize}
\end{proof}

On the other hand, when using a bidirectional type synthesiser to implement type synthesis~(\cref{thm:implementation}), our completeness is much simpler to use than annotatability, which requires the bidirectional type synthesiser to produce more complex evidence when the synthesis fails.
Annotatability also does not help the user to deal with missing annotations like (generalised) mode preprocessing does: although annotatability seems capable of determining where annotations are missing and even filling them in correctly, its antecedent requires a typing derivation, which is what the user is trying to construct and does not have yet.
Therefore we believe that the notion of annotatability is too complex for what it achieves, and our theory offers simpler and more useful alternatives.
